[{"id":"r9io6e","date":"2021-12-05","name":"[D] Simple Questions Thread","name_author":"AutoModerator","score":4,"subreddit_name":"MachineLearning","url":"https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/r9io6e\/d_simple_questions_thread\/","num_comments":28,"text":"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!\n\nThread will stay alive until next one so keep posting after the date in the title.\n\nThanks to everyone for answering questions in the previous thread!"},{"id":"r4e8he","date":"2021-11-28","name":"[D] Machine Learning - WAYR (What Are You Reading) - Week 126","name_author":"ML_WAYR_bot","score":24,"subreddit_name":"MachineLearning","url":"https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/r4e8he\/d_machine_learning_wayr_what_are_you_reading_week\/","num_comments":6,"text":"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.\n\nPlease try to provide some insight from your understanding and please don't post things which are present in wiki.\n\nPreferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.\n\nPrevious weeks :\n\n|1-10|11-20|21-30|31-40|41-50|51-60|61-70|71-80|81-90|91-100|101-110|111-120|121-130|\n|----|-----|-----|-----|-----|-----|-----|-----|-----|------|-------|-------|-------|\n|[Week 1](https:\/\/www.reddit.com\/4qyjiq)|[Week 11](https:\/\/www.reddit.com\/57xw56)|[Week 21](https:\/\/www.reddit.com\/60ildf)|[Week 31](https:\/\/www.reddit.com\/6s0k1u)|[Week 41](https:\/\/www.reddit.com\/7tn2ax)|[Week 51](https:\/\/reddit.com\/9s9el5)|[Week 61](https:\/\/reddit.com\/bfsx4z)|[Week 71](https:\/\/reddit.com\/d7vno3)|[Week 81](https:\/\/reddit.com\/f1f0iq)|[Week 91](https:\/\/reddit.com\/hlt38o)|[Week 101](https:\/\/reddit.com\/k81ywb)|[Week 111](https:\/\/reddit.com\/myg8sm)|[Week 121](https:\/\/reddit.com\/pmzx3g)|||||||||||\n|[Week 2](https:\/\/www.reddit.com\/4s2xqm)|[Week 12](https:\/\/www.reddit.com\/5acb1t)|[Week 22](https:\/\/www.reddit.com\/64jwde)|[Week 32](https:\/\/www.reddit.com\/72ab5y)|[Week 42](https:\/\/www.reddit.com\/7wvjfk)|[Week 52](https:\/\/reddit.com\/a4opot)|[Week 62](https:\/\/reddit.com\/bl29ov)|[Week 72](https:\/\/reddit.com\/de8h48)|[Week 82](https:\/\/reddit.com\/f8fs6z)|[Week 92](https:\/\/reddit.com\/hu6zq9)|[Week 102](https:\/\/reddit.com\/kh27nx)|[Week 112](https:\/\/reddit.com\/n8m6ds)|[Week 122](https:\/\/reddit.com\/pw14z5)||\n|[Week 3](https:\/\/www.reddit.com\/4t7mqm)|[Week 13](https:\/\/www.reddit.com\/5cwfb6)|[Week 23](https:\/\/www.reddit.com\/674331)|[Week 33](https:\/\/www.reddit.com\/75405d)|[Week 43](https:\/\/www.reddit.com\/807ex4)|[Week 53](https:\/\/reddit.com\/a8yaro)|[Week 63](https:\/\/reddit.com\/bqlb3v)|[Week 73](https:\/\/reddit.com\/dkox1s)|[Week 83](https:\/\/reddit.com\/ffi41b)|[Week 93](https:\/\/reddit.com\/iaz892)|[Week 103](https:\/\/reddit.com\/kpsxtc)|[Week 113](https:\/\/reddit.com\/njfsc6)|[Week 123](https:\/\/reddit.com\/q5fi12)||\n|[Week 4](https:\/\/www.reddit.com\/4ub2kw)|[Week 14](https:\/\/www.reddit.com\/5fc5mh)|[Week 24](https:\/\/www.reddit.com\/68hhhb)|[Week 34](https:\/\/www.reddit.com\/782js9)|[Week 44](https:\/\/reddit.com\/8aluhs)|[Week 54](https:\/\/reddit.com\/ad9ssz)|[Week 64](https:\/\/reddit.com\/bw1jm7)|[Week 74](https:\/\/reddit.com\/dr6nca)|[Week 84](https:\/\/reddit.com\/fn62r1)|[Week 94](https:\/\/reddit.com\/ijjcep)|[Week 104](https:\/\/reddit.com\/kzevku)|[Week 114](https:\/\/reddit.com\/ntu6lq)|[Week 124](https:\/\/reddit.com\/qjxfu9)||\n|[Week 5](https:\/\/www.reddit.com\/4xomf7)|[Week 15](https:\/\/www.reddit.com\/5hy4ur)|[Week 25](https:\/\/www.reddit.com\/69teiz)|[Week 35](https:\/\/www.reddit.com\/7b0av0)|[Week 45](https:\/\/reddit.com\/8tnnez)|[Week 55](https:\/\/reddit.com\/ai29gi)|[Week 65](https:\/\/reddit.com\/c7itkk)|[Week 75](https:\/\/reddit.com\/dxshkg)|[Week 85](https:\/\/reddit.com\/fvk7j6)|[Week 95](https:\/\/reddit.com\/is5hj9)|[Week 105](https:\/\/reddit.com\/l9lvgs)|[Week 115](https:\/\/reddit.com\/o4dph1)|[Week 125](https:\/\/reddit.com\/qtzbu1)||\n|[Week 6](https:\/\/www.reddit.com\/4zcyvk)|[Week 16](https:\/\/www.reddit.com\/5kd6vd)|[Week 26](https:\/\/www.reddit.com\/6d7nb1)|[Week 36](https:\/\/www.reddit.com\/7e3fx6)|[Week 46](https:\/\/reddit.com\/8x48oj)|[Week 56](https:\/\/reddit.com\/ap8ctk)|[Week 66](https:\/\/reddit.com\/cd7gko)|[Week 76](https:\/\/reddit.com\/e4nmyk)|[Week 86](https:\/\/reddit.com\/g4eavg)|[Week 96](https:\/\/reddit.com\/j0xr24)|[Week 106](https:\/\/reddit.com\/ljx92n)|[Week 116](https:\/\/reddit.com\/odrudt)||\n|[Week 7](https:\/\/www.reddit.com\/52t6mo)|[Week 17](https:\/\/www.reddit.com\/5ob7dx)|[Week 27](https:\/\/www.reddit.com\/6gngwc)|[Week 37](https:\/\/www.reddit.com\/7hcc2c)|[Week 47](https:\/\/reddit.com\/910jmh)|[Week 57](https:\/\/reddit.com\/auci7c)|[Week 67](https:\/\/reddit.com\/cj0kyc)|[Week 77](https:\/\/reddit.com\/eb4lxk)|[Week 87](https:\/\/reddit.com\/gcx3uf)|[Week 97](https:\/\/reddit.com\/j9cbfs)|[Week 107](https:\/\/reddit.com\/luqbxl)|[Week 117](https:\/\/reddit.com\/omy345)||\n|[Week 8](https:\/\/www.reddit.com\/53heol)|[Week 18](https:\/\/www.reddit.com\/5r14yd)|[Week 28](https:\/\/www.reddit.com\/6jgdva)|[Week 38](https:\/\/www.reddit.com\/7kgcqr)|[Week 48](https:\/\/reddit.com\/94up0g)|[Week 58](https:\/\/reddit.com\/azjoht)|[Week 68](https:\/\/reddit.com\/cp1jex)|[Week 78](https:\/\/reddit.com\/ehbfst)|[Week 88](https:\/\/reddit.com\/glm6sv)|[Week 98](https:\/\/reddit.com\/jhzz9v)|[Week 108](https:\/\/reddit.com\/m52u5z)|[Week 118](https:\/\/reddit.com\/ovz52j)||\n|[Week 9](https:\/\/www.reddit.com\/54kvsu)|[Week 19](https:\/\/www.reddit.com\/5tt9cz)|[Week 29](https:\/\/www.reddit.com\/6m9l1v)|[Week 39](https:\/\/www.reddit.com\/7nayri)|[Week 49](https:\/\/reddit.com\/98n2rt)|[Week 59](https:\/\/reddit.com\/b50r5y)|[Week 69](https:\/\/reddit.com\/cvde5a)|[Week 79](https:\/\/reddit.com\/entcxy)|[Week 89](https:\/\/reddit.com\/gu5t0d)|[Week 99](https:\/\/reddit.com\/jqjgo2)|[Week 109](https:\/\/reddit.com\/mf8m6u)|[Week 119](https:\/\/reddit.com\/p50knh)||\n|[Week 10](https:\/\/www.reddit.com\/56s2oa)|[Week 20](https:\/\/www.reddit.com\/5wh2wb)|[Week 30](https:\/\/www.reddit.com\/6p3ha7)|[Week 40](https:\/\/www.reddit.com\/7qel9p)|[Week 50](https:\/\/reddit.com\/9cf158)|[Week 60](https:\/\/reddit.com\/bakew0)|[Week 70](https:\/\/reddit.com\/d1g1k9)|[Week 80](https:\/\/reddit.com\/euctyw)|[Week 90](https:\/\/reddit.com\/hddf7j)|[Week 100](https:\/\/reddit.com\/jz3evt)|[Week 110](https:\/\/reddit.com\/moy40m)|[Week 120](https:\/\/reddit.com\/pe2idh)||\n\nMost upvoted papers two weeks ago:\n\n\/u\/CatalyzeX_code_bot: [Paper link](https:\/\/arxiv.org\/abs\/2012.09841)\n\nBesides that, there are no rules, have fun."},{"id":"rbisbe","date":"2021-12-08","name":"Player of Games - Deepmind.","name_author":"chillinewman","score":107,"subreddit_name":"MachineLearning","url":"https:\/\/arxiv.org\/pdf\/2112.03178.pdf","num_comments":21,"text":""},{"id":"rboh2r","date":"2021-12-08","name":"[D] Are transformers overhyped?","name_author":"The_deepest_learner","score":16,"subreddit_name":"MachineLearning","url":"https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/rboh2r\/d_are_transformers_overhyped\/","num_comments":24,"text":"I've been wondering about this for a pretty long time since I've never seen anybody say anything bad about transformers, while to me, they seemed pretty flawed from the moment I've read the paper. I'm in no way an ML expert. I'm only an aspiring PhD student, who's not even specializing in NLP, so if I'm any way wrong I'd really like to hear it.\n\ntl;dr: I believe that transformers are, in the long term, a pretty small contribution to the world of NLP, and may even be damaging due to shifting the focus of the research community in the wrong direction. Why? They don't address the long-term dependency problem.\n\nBefore transformers NLP used to be dominated by RNNs and specifically the encoder-decoder architecture. In the case of translation, the encoder would encode the input sentence in a fixed-length vector and the decoder would then decode this vector into an output translated sentence. Now transformers also use encoder-decoder architecture, but there is one big difference. For RNNs, encoding and decoding actually happens at every step of the way. Words (I know it's tokens but I'll call them words) are inserted sequentially into the RNN. For every single word, the encoder RNN had to look at the current encoding vector and the input word and then choose how to update the encoding vector in a meaningful way.\n\nThe problem with this approach, which I'll call long-term dependency, arises when the RNN has to look at a very long sequence of words. Humans can easily distill the information that they've read and remember only the important bits, for example, the name of a character that was mentioned 5 pages ago. But RNN models had trouble encoding what happened 5 sentences ago. The research community starts solving this problem with the original attention paper, but then transformers come out.\n\nSo out comes the transformer and starts dominating the NLP world. What does the transformer do? It is a huge model that, when encoding simply takes 512 input words (or some other arbitrary number) and looks at all of them simultaneously. And it works wonders. Look, the transformer can remember what happened 5 sentences ago because the previous 5 sentences combined have less than 512 words, hooray. Can it remember what happened 10 sentences ago though? Uh well... no. Can we improve it in some way to solve the long-term dependency problem? Well, we can be smart about which sentences we feed into it, but that means we still have to distill information from a large body of text so... we're back at the beginning.\n\nIt's obvious that we have to solve the long-term dependency problem if we ever hope to achieve human-like NLP models, and to me, it seems that transformers do nothing to solve this problem. So why are they dominating the field of NLP research? Maybe the optimal solution will include a combination of both the transformer and some other model for information distillation, but if we still need to solve the long-term dependency problem why are throwing out RNNs so quickly?"},{"id":"razsj2","date":"2021-12-07","name":"[D] Why do people \u201cread\u201d as many papers as possible?","name_author":"DaBeastGeek","score":513,"subreddit_name":"MachineLearning","url":"https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/razsj2\/d_why_do_people_read_as_many_papers_as_possible\/","num_comments":104,"text":"I\u2019ve got a few colleagues who always claim to be reading papers, but the way they \u201cread\u201d is so damn superficial. \n\nAs an example, I had just finished fully reading\/comprehending a paper, and I won\u2019t lie, took me a solid couple days to understand everything fully and reading things multiple times. \n\nMeanwhile, in the daily meetings we have I mention the paper and how we should try and use some of their components in our own work, and someone says, \u201coh ya, I read that in like 15 mins\u201d. So we decide to have an impromptu discussion on it and Jesus Christ, I swear the only thing he read was the abstract and maybe glanced at the network architecture. \n\nI\u2019m sorry this is turning into an rant, it just really grates my nerves when people say they read something and in reality all they did was look at the abstract. \n\nI\u2019m a firm believe that reading, comprehending and fully understand 1 single \u201ckey\u201d paper from whatever field you\u2019re studying, is a much better investment of your time than skimming through 100 regurgitated ideas.\n\nEdit: guys just to clarify, I do believe in skimming abstracts and looking for interesting papers. I go through dozens a day myself.  You\u2019d be lost otherwise haha. I take issue though when someone claims they\u2019ve \u201cread\u201d something when all they\u2019ve done is gone through the abstract, and glanced through it."},{"id":"rbc8s5","date":"2021-12-08","name":"[D] How do you organize\/track your reading list?","name_author":"dogs_like_me","score":15,"subreddit_name":"MachineLearning","url":"https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/rbc8s5\/d_how_do_you_organizetrack_your_reading_list\/","num_comments":22,"text":"I feel like at any given time, I have a million tabs open on like 20 different browser instances, each tab linking an arxiv abstract I want to read. Don't even get me started on my github stars and browser bookmarks.\n\nHow do you keep track of stuff you want to read? How do you keep track of things you've read that you want to be able to dig up later? How do you tie in your notes?\n\nNeurIPS has barely started and I already feel like I'm drowning in stuff I want to dive into and not lose immediately afterwards.\n\nAre there some like top-secret tools only the hip grad students know about or something? How the hell do you collect and organize the stuff you want to read and\/or recently read?\n\nPlease tell me there's a better way"},{"id":"rbqomm","date":"2021-12-08","name":"[D] How should a training dataset be distributed?","name_author":"Reynso","score":1,"subreddit_name":"MachineLearning","url":"https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/rbqomm\/d_how_should_a_training_dataset_be_distributed\/","num_comments":0,"text":"Sorry if this is a beginner question.\nSuppose i'm building a text-to-speech model. I was wondering if my training dataset should be \"realistically\" distributed (i.e. same distribution as the data it will be used on), or should it be uniformly distributed to make sure it performs well on all kind of sentences.\nThanks for your insight."},{"id":"rbn0fh","date":"2021-12-08","name":"[P] Ivis: Dimensionality Reduction In Very Large Datasets Using Siamese Networks","name_author":"EducationalCicada","score":2,"subreddit_name":"MachineLearning","url":"https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/rbn0fh\/p_ivis_dimensionality_reduction_in_very_large\/","num_comments":0,"text":"[https:\/\/github.com\/beringresearch\/ivis](https:\/\/github.com\/beringresearch\/ivis)"},{"id":"rbq46a","date":"2021-12-08","name":"[D] Knowing how features affect the target variables in the neural network","name_author":"Ill-Ad-106","score":0,"subreddit_name":"MachineLearning","url":"https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/rbq46a\/d_knowing_how_features_affect_the_target\/","num_comments":4,"text":"I am currently working on blood glucose forecasting using LSTMs. Of course, insulin decreases blood glucose and carbohydrates increase blood glucose. Is there any way to feed this in the network? I am using Keras and simply inputting insulin and carbs as an array in the network, but I know for certain how each feature will affect the forecast (i.e. negative and positive correlation). How can I incorporate this is the network? \n\nI am sure this isn't specific to my problem. In most ML prediction\/forecasting problems, we would know if a particular feature increases or decreases our target variable. How can we utilise this information?"},{"id":"rbrs4k","date":"2021-12-08","name":"Need any insights possible on this A.I project. People of reddit save my soul! [Project]","name_author":"That-Ad767","score":0,"subreddit_name":"MachineLearning","url":"https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/rbrs4k\/need_any_insights_possible_on_this_ai_project\/","num_comments":1,"text":"Hey, folks of Reddit! \n\n**I'm in a dire situation. I need to complete my college project on artificial intelligence.**\n\nBut I'm so lost, I don't know how to build this as I don't have solid experience in A.I. I've only ever built apps and websites. \n\n**The project given to me is \"detecting seat occupancy using A.I\"**\n\nSo for example, if I take a photo of 9 seats (3 of which are occupied by my friends), The A.I should be able to tell that 3 out of 9 seats are occupied. \n\nHow can I go about doing this? I can some grasp terms in deep learning since I've played around with it a tiny bit. \n\nAny help would be greatly appreciated. Cheers!"}]